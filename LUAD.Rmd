---
title: "LUAD"
author: "Cihan"
date: "27 12 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#LIBRARIES
We first load necessary libraries
```{r}
#install.packages("naniar")
#install.packages("cowplot") 
#install.packages("superml")
#install.packages("VIM")
library(VIM)
library(naniar)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(corrplot)
library(superml) #Label Encoding
library(caret) #dummyVars for One Hot Encoding
```
# DATA

### Load the Data
```{r}
missing.values <- c("N/A","na","?","", "NaN", "not reported", "NA")

#data <- read.csv("C:\\Users\\sakma\\Desktop\\Ders Kayıtları\\Data Mining\\Final Project\\TCGA-LUAD_clinical.csv", na.strings=missing.values)

data <- read.csv("C:\\Users\\Gaye\\OneDrive\\Masaüstü\\dersKayıtü\\DataScience\\DataScienceFinal\\TCGA-LUAD_clinical.csv", na.strings=missing.values)

```

## 1.2.3) Data Analysis and Visualization **We should give some information about data....**
```{r cars}
summary(data)
```

As we can see from the summary() result our variables have different types of classes. Also, most of them include NA values.

Let's run str() to get information about types of features.
```{r cars}
str(data) #Types of features
```
From the str() result, we understand that our features includes different types of classes also we can see lots of NA values.


Let's take a final look at the structure and contents of our data in a visual and more entertaining way.

```{r}
library(visdat)
vis_dat(data)
```

As we can see from the table, we have many NA values, but some variables look full NA. We cannot say that these will be of use to us in the later stages.

# PREPROCESSING

## At this step firstly we're going to drop the fully NaN columns. Because these columns are not useful for us. Maybe we can show these full nan columns in a fashionable way.

**we are going to get a rough glimpse on the missingness situation with the pretty neat naniar package**

**Note:** we can go through https://towardsdatascience.com/smart-handling-of-missing-data-in-r-6425f8a559f2 and leave some comments for our dataset.
```{r}
# Are there missing values in the dataset?
any_na(data)
```

```{r}
# How many?
n_miss(data)
prop_miss(data)

```

```{r}

# Which variables are affected?
data %>% is.na() %>% colSums()

```

```{r}
# Drop full NA variables and append the new data frame to the data_new
data_new <- data[ , colSums(is.na(data)) < nrow(data)]
# Take a look our new dataframe
summary(data_new)

```
Now we got rid of the full NA variables but we still have some variables that contain NA values.

Let's take a closer look at to the missing values with tables.
```{r}
# Get number of missings per variable (n and %)
miss_var_summary(data_new)
#miss_var_table(data)

```

By looking at missing summary per variable, we notice that especially the “year_of_death”-Variable has the highest amount of missings among all variables in the dataset. The variable represents the “year of death of individuals.” 

We can transfer this summary to a visual representation like this:

```{r}
# Which variables contain the most missing variables?
gg_miss_var(data_new)
```

To get a better understanding whether or not the data are missing at random, we are going to visualize the locations of missing values across all variables.
```{r}
# Where are missings located?
vis_miss(data_new) + theme(axis.text.x = element_text(angle=80))
```
We can see where the missing values are “clustered” and it seems to match our findings from our previous overview on the presence of missing values per variable.

We can see that the presence of some values in some variables coincides with the absence of others. We can see the *days_to_last_follow_up* and *days_to_death* columns that are an example of missing with the existence of others.

Now is the presence of some values are related with missingness in other variables? Let’s find out.

```{r}
gg_miss_upset(data_new)
```
The results are compatible with the observation that there is a substantial number of cases in which some missingness happen to occur across certain variables.

From above result we can say that **There is a direct realtionship between *days_to_last_follow_up* and *days_to_death* columns**. Of course, there are other variables that meet this condition. But the binary that stood out compared to the numeric values was *days_to_last_follow_up* and *days_to_death*.

Before doing anything to missing values, we need to know the pattern of occurrence of missing values. The most effective way to do this is to draw a plot.

In order to examine the pattern between the variables containing the missing value by drawing a plot, let's take only the variables with at least one missing value from all the variables
```{r}
columns_with_na<-names(which(colSums(is.na(data_new)) > 0))
#columns_with_na

data_w_na <- subset(data_new, select = c(columns_with_na))
head(data_w_na)
```

Here first we change the variable name of *days_to_last_follow_up* to *days_to_lfu* in order to fit the name in the plot. This variable gives us information about how long alive people have their check-in and there is no information about the dead patients.

```{r}
names(data_w_na)[names(data_w_na) == 'days_to_last_follow_up'] <- 'days_to_lfu'
```

The missing patterns can be visualized using VIM package's **aggr** function.

```{r}
aggr(data_w_na, col=c('lightblue','red'),  sortVars=TRUE, labels=names(data_w_na), cex.axis=.5, gap=3, ylab=c("Histogram of missing data","Pattern"))

```
The histogram shows the proportion of missingness and the plot on the right gives the frequencies for different combinations of variables missing where. Blue refers to observed data and red to missing data.

From the above plot, again we can see presence of some values that related with missingness in other variables.

***
## Y.a) Missing Value Imputations

In the code below we'll drop the row, if there is a case like that if patient vital status is dead but days_to_death is 0.
As we can see we only have one patient with that situation in LUAD and we dropped that patient.
```{r}
paste("Before Missing value imputation we're going to keep the data with missing values as 'data_new_w_na'")
data_w_na <- data_new


filter(data_new, days_to_death == 0 & vital_status == "dead")
nrow(data_new)

drop_rows <-subset(data_new, (vital_status=="dead" & days_to_death==0))
c <-nrow(drop_rows)
paste("Count of patient with vital status is dead but days_to_death is 0 is:", c)

data_new <- data_new[!data_new$submitter_id %in% drop_rows$submitter_id,]
filter(data_new, days_to_death == 0 & vital_status == "dead")
nrow(data_new)


```

As we mentioned above **There is a direct realtionship between *days_to_last_follow_up* and *days_to_death* columns** but let's take a closer look to their relationship.

```{r}
to.death.vs.last.follow.up <- subset(data_new, select = c(days_to_last_follow_up,days_to_death))
aggr(to.death.vs.last.follow.up, col=c('lightblue','red'),  sortVars=TRUE, numbers=TRUE, cex.numbers=0.9,labels=names(to.death.vs.last.follow.up), cex.axis=.5, gap=3, ylab=c("Histogram of missing data","Pattern"))
```
From above plot we can say that the 63.1% of the missing values in **days_to_death** is present in **days_to_last_follow_up** and 22.5% of the missing values in **days_to_last_follow_up** is present in **days_to_death**. 
- Now, Now we'll fill the NaN values in days_to_death column with the days_to_last_follow_up. Subsequently, we're going to drop the days_to_last_follow_up.
```{r}
data_new$days_to_death[is.na(data_new$days_to_death)] <- data_new$days_to_last_follow_up[is.na(data_new$days_to_death)]  # Replace NA values

#Drop the days_to_last_follow_up.
data_new <- subset(data_new, select = -c(days_to_last_follow_up))
gg_miss_upset(data_new)
```
As we can see we successfully merge these two features.

***

- '''We can drop year_of_birth column because we already have days_to_birth which is more suitable for our algorithm'''

- '''In 'state' column all the variables are same for all rows as 'released'. We'll drop it''' (*SHOW THAT*)

```{r}
#unique_data <- sapply(data_new, function(x) length(unique(x)))
unique_data <- data_new %>% summarise_each(funs(n_distinct))
unique_data
```

- '''Drop updated_datetime because there is no special information about patient status'''(We don't have to prove it with table or plot it's just not useful)
- '''Also we should remove the ids. Because there is nothing special for these ids. These are just random variables.'''
  - id_columns = {"diagnosis_id", "exposure_id", "demographic_id", "treatment_id", "bcr_patient_barcode"}
- 76% of the year_of_death column is missing. We're going to drop that column because we already have the information of days_to_death which refers to information of lifetime of the patient before dying. 


```{r}
data_new <- subset(data_new, select = -c(year_of_birth, state, updated_datetime, diagnosis_id, exposure_id, demographic_id, treatment_id, bcr_patient_barcode, year_of_death))
```

After dropping unnecessary variables which is fully NA and variables that contain only the same values, this is our last version of data.

```{r}
library(visdat)
vis_dat(data_new)
```
***


#### Now we will fill the numeric NaN variables with the mean of columns.

- numeric_columns = {"age_at_diagnosis", "days_to_birth","years_smoked","cigarettes_per_day"}
**Note:** We can show the missing values with dot plot and after imputing the data we can do the same thing again for all 4 features.
**Note:** We can also try to fill the missing values after splitting test and train data and impute the missing values separately for test and train data.
```{r}
## Try to do the same operation with less code. Try sth fancy.
data_new$age_at_diagnosis[is.na(data_new$age_at_diagnosis)] <- mean(data_new$age_at_diagnosis, na.rm = TRUE)
data_new$days_to_birth[is.na(data_new$days_to_birth)] <- mean(data_new$days_to_birth, na.rm = TRUE)
data_new$years_smoked[is.na(data_new$years_smoked)] <- mean(data_new$years_smoked, na.rm = TRUE)
data_new$cigarettes_per_day[is.na(data_new$cigarettes_per_day)] <- mean(data_new$cigarettes_per_day, na.rm = TRUE)

# Which variables contain the most missing variables?
gg_miss_var(data_new)
```

We can see that we successfully imputed these features with mean of the features.

- '''Now we have null values in **ethnicity** and **race** columns. These columns are highly related with cancer genetic we will keep them as unknown'''
```{r}
 
head(data_new)

data_new <- data_new %>%
    mutate(ethnicity = if_else(is.na(ethnicity), "unknown", ethnicity))
data_new <- data_new %>%
    mutate(race = if_else(is.na(race), "unknown", race))

head(data_new)
```

  - We can show the new distribution of the **ethnicity** and **race** columns in a plot.
```{r}
p1 <- ggplot(data_new, aes(x=reorder(ethnicity, ethnicity, function(x)-length(x)))) +
geom_bar(fill='orange') +  labs(x='Ethnicity', y="")
p2 <- ggplot(data_new, aes(x=reorder(race, race, function(x)-length(x)))) +
geom_bar(fill='purple') +  labs(x='Race', y="")

require(gridExtra)
grid.arrange(p1, p2, nrow=2)


```



(There are still some NaN values in 'tumor_stage'(8) and 'days_to_death'(9). We'll drop them)
```{r}
sapply(data_new,function(x) sum(is.na(x)))
vars <- c("tumor_stage", "days_to_death")
data_new <- data_new %>% drop_na(vars, any_of(vars))
sapply(data_new,function(x) sum(is.na(x)))
```

As we can see, we don't have any null values in the dataset anymore.
```{r}
gg_miss_var(data_new)
```

## 4) Multicollinearity

- Let's plot the Correlation Coefficient Matrix for multicollinearity check. But we can apply the Correlation Coefficient Matrix only to numeric columns. Also, we need to check for non-numeric features for multicollinearity. We're going to use a function to check if there are identical columns in the data frame with non-numeric columns. To be able to check for identical features, we're going to print a list that consists of two columns. If a feature has an identical feature second column will show the feature which is identical to the first one.
```{r}
matrix <- data_new %>%
    select_if(is.numeric) %>%
    cor(.)
corrplot(matrix, method = "color", addCoef.col="black", order = "alphabet", number.cex=0.6, title = "Correlation Coefficient Matrix",
         tl.cex=0.6, tl.col = "black")

tt <- lapply(unique(as.list(data_new)), function(x) {colnames(data_new)[as.list(data_new) %in% list(x)]})
t(sapply(tt, "length<-", max(lengths(tt))))
```
*Note*: The above correlation matrix shows that there are no highly correlated features and due to there are no multicollinearity for numeric features. On the other hand, for the non-numeric features it seems that tissue_or_organ_of_origin and site_of_resection_or_biopsy are identical features. Therefore, we're going to drop **tissue_or_organ_of_origin**.

```{r}
data_new <- subset(data_new, select = -c(tissue_or_organ_of_origin))
```


## ENCODING

- We're going to convert our String Binary columns into Numeric Binary columns with LabelEncdoing. But first we're going to convert these three Character features into Factor.
  - Our Binary String Columns are: vital_status, gender, disease
```{r}

data_new$vital_status <- as.factor(data_new$vital_status) 
data_new$gender <- as.factor(data_new$gender) 
data_new$disease <- as.factor(data_new$disease) 

head(data_new)

label <- LabelEncoder$new()
data_new$vital_status <- label$fit_transform(data_new$vital_status) #0:Alive, 1:Dead
data_new$gender <- label$fit_transform(data_new$gender) #1:Male, 0:Female
data_new$disease <- label$fit_transform(data_new$disease) #We're going to drop it.

head(data_new)
```
- **tumor_stage** is an ordinal column. We'll encode it manuelly. We can get the unique values for the feature and show them.
  - The order is going to be as follows:
    - tumor_stage_map = {'stage i': 1, 'stage ia': 2, 'stage ib': 3, 'stage ii': 4, 'stage iia': 5, 'stage iib': 6, 'stage iiia':7, 'stage iiib':8, 'stage iv':9 }

```{r}
head(data_new)
p1 <- ggplot(data_new, aes(x=reorder(tumor_stage, tumor_stage, function(x)-length(x)))) +
geom_bar(fill='orange') +  labs(x='Tumor Stages', y="")
p1


data_new$tumor_stage <- as.numeric(as.factor(data_new$tumor_stage))


str(data_new$tumor_stage)
typeof(data_new$tumor_stage)
head(data_new)
```



- We'll apply OneHotEncoding for Nominal Columns with **dummyVars** function in the *caret* package.
  - Our categorical columns are: **primary_diagnosis, morphology, site_of_resection_or_biopsy, race, ethnicity**
```{r}
categorical.cols <- subset(data_new, select=c(primary_diagnosis, morphology, site_of_resection_or_biopsy, race, ethnicity))
head(categorical.cols)

dmy <- dummyVars(" ~ .", data = categorical.cols)
categorical.cols <- data.frame(predict(dmy, newdata = categorical.cols))
head(categorical.cols)
```

  - We'll drop categorical columns and then we'll merge the new One Hot Encoded version of categorical.cols to the dataset.
```{r}
categorical.cols$submitter_id <- data_new$submitter_id #We need that submitter_id column to merge two dataframes.
data_new <- subset(data_new, select = -c(primary_diagnosis, morphology, site_of_resection_or_biopsy, race, ethnicity))
data_new <- merge(data_new, categorical.cols, by="submitter_id")
```
  
  
- Drop vital_status(alive or dead) and disease. In real life we don't need vital_status feature. On the other hand **disease** stands for type of Lung Cancer(LUAD or LUSC) in our case all the patients are LUAD cancer, therefore we don't need to keep that column. Also we do not need **submitter_id** feature any more.
```{r}
data_new <- subset(data_new, select = -c(vital_status, disease, submitter_id))
```

## Classifiyng Algorithm

We would like to classify the patients such as they will live more than five years or not according to their *days_to_death* feature. Therefore, if the patient's remaining life more than 5 years we assumed the patient will live long(assign 1), otherwise short(assign 0). Then convert it to the factor.
```{r}
head(data_new)
data_new$days_to_death <- ifelse(data_new$days_to_death>1825, 'low risk', 'high risk')
data_new$days_to_death <- as.factor(data_new$days_to_death)
head(data_new)
str(data_new)
```




## Z.a) Imbalance Check

## Let's see if we have a balanced classes or not for the target data(*days_to_death*).
```{r}
p1 <- ggplot(data_new, aes(x=reorder(days_to_death, days_to_death, function(x)-length(x)))) +
geom_bar(fill='orange') +  labs(x='Classes', y="")
p1
```
Before applying Oversampling algorithm to data let's first split the data into two parts, train(%80) and test(%20) dataset.
```{r}
set.seed(12345)
#samples <- data_new$days_to_death %>% createDataPartition(p = 0.8, list = FALSE)

#train  <- data_new[samples, ]
#test <- data_new[-samples, ]

#paste("Size of training set:",nrow(train),",",ncol(train))
#paste("Size of test set:",nrow(test),",",ncol(test))
```


It seems that we have a serious imbalance class classification problem. We're going to use the SMOTE algorithm, the SMOTE is an oversampling technique which synthesizes a new minority instance between a pair of one minority instance and one of its K nearest neighbor.
```{r}
paste("We're going to keep the imbalance data as 'imbalance_data' and 'data_new' is going to be our balanced data.")

imbalance_data <- data_new


data_new <- SMOTE(subset(data_new, select = -c(days_to_death)),subset(data_new, select = c(days_to_death)),K=5)$data



names(data_new)[names(data_new) == 'class'] <- 'days_to_death'


p1 <- ggplot(data_new, aes(x=reorder(days_to_death, days_to_death, function(x)-length(x)))) +
geom_bar(fill='orange') +  labs(x='Classes(0: live < 5 years, 1: live > 5 years)', y="Count") + ggtitle("Balanced Data")
p1
```
After applying SMOTE now we have a balanced data. 

## 5) PCA
First, we apply PCA keeping all components equal to the original number of dimensions and see how well PCA captures the variance of our data.
```{r}
X_data <- subset(data_new, select = -c(days_to_death))
y_data <- subset(data_new, select = c(days_to_death)) # our target

#Now we can apply PCA:
X_data_pca <- prcomp(X_data,center = TRUE, scale. = TRUE)
summary(X_data_pca)

#We are going to apply same step for X test:
#which(apply(X.test, 2, var)==0)
#X.test <- X.test[ , which(apply(X.test, 2, var) != 0)]
#X.test.pca <- prcomp(X.test,center = TRUE, scale. = TRUE)

```

Because the point of PCA is to significantly reduce the number of variables, we want to use the smallest number of principal components possible to explain most of the variability.

The most common technique for determining how many principal components to keep is eyeballing the scree plot. In a scree plot the ‘arm-bend’ represents a decrease in cumulative contribution also it show the percentage of variances explained by each principal component.
```{r}
screeplot(X_data_pca, type = "l", npcs = 25)
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Eigenvalue = 1"),
       col=c("red"), lty=5, cex=0.6)

```
The above plot shows the bend at the eighteenth principal component, it is useful to decide how many PCs to retain for further analysis. In this case we have 18 PCs and we can see that the first 18 PCs explain most of the variability in the data.

```{r}
cumpro <- cumsum(X_data_pca$sdev^2 / sum(X_data_pca$sdev^2))
plot(cumpro[0:30], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative Variance")
abline(v = 20, col="blue", lty=5)
abline(h = 0.89271, col="blue", lty=5)
legend("topleft", legend=c("Cut-off @ PC6"),
       col=c("blue"), lty=5, cex=0.6)
```
From above plot we can see that nearly with 20 PCs we can explain %89.271 of total variance.

Let's see the importance of first 20 PCs out of 42:
```{r}
X_data_pca_20 <- prcomp(subset(X_data), center = TRUE, scale = TRUE, rank.= 20)
summary(X_data_pca_20)
```

```{r}
M <-cor(X_data_pca_20$x)
corrplot(M, method = "color", addCoef.col="brown", order = "FPC", number.cex=0.9, 
         tl.cex=0.7, tl.col = "brown", tl.srt = 90)

```
```{r}
transformed_data<- cbind(X_data_pca_20$x,y_data)
head(transformed_data)
```
## 6) LOGISTIC REGRESSION
```{r}
training_samples <- data_new$days_to_death %>% createDataPartition(p = 0.8, list = FALSE)


data_new$days_to_death <- ifelse(data_new$days_to_death == "low risk", 0, 1)
data_new$days_to_death <- as.factor(data_new$days_to_death)

train_data  <- data_new[training_samples, ]
test_data <- data_new[-training_samples, ]

model <- glm(days_to_death ~.,family=binomial,data=train_data)

fitted.results <- predict(model,subset(test_data,select=-c(days_to_death)),type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)
misClasificError <- mean(fitted.results != test_data$days_to_death)

print(paste('Test Accuracy:',1-misClasificError))

```


## 6.X) LOGISTIC REGRESSION WTIH PCA(*Galiba çalışmıyor kontrol et*)

We transform original data to the apllied pca data then split it into train and test set in order to apply logistic regression.
```{r}
training_samples_pca <- transformed_data$days_to_death %>% createDataPartition(p = 0.8, list = FALSE)


transformed_data$days_to_death <- ifelse(transformed_data$days_to_death == "low risk", 0, 1)
transformed_data$days_to_death <- as.factor(transformed_data$days_to_death)

train_data_pca  <- transformed_data[training_samples_pca, ]
test_data_pca <- transformed_data[-training_samples_pca, ]

#Now we can build our linear regression model with our new data
model_pca <- glm(days_to_death ~.,family=binomial,data=train_data_pca)

fitted_results <- predict(model_pca,subset(test_data_pca,select=-c(days_to_death)),type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)
misClasificError <- mean(fitted.results != test_data_pca$days_to_death)

print(paste('Test Accuracy:',1-misClasificError))
```

## 6.Y.b) LOGISTIC REGRESSION WTIH MISSING VALUES (*ERROR*)
```{r}
training_samples_missing <- data$days_to_death %>% createDataPartition(p = 0.8, list = FALSE)


train_data_missing  <- data[training_samples_missing, ]
test_data_missing <- data[-training_samples_missing, ]

model_imbalance <- glm(days_to_death ~.,family=binomial,data=train_data_missing)

fitted_results <- predict(model_imbalance,subset(test_data_missing,select=-c(days_to_death)),type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)
misClasificError <- mean(fitted.results != test_data_missing$days_to_death)

print(paste('Test Accuracy:',1-misClasificError))

```

## 6.Z.b) LOGISTIC REGRESSION WTIH IMBALANCED
```{r}
training_samples_imbalance <- imbalance_data$days_to_death %>% createDataPartition(p = 0.8, list = FALSE)


imbalance_data$days_to_death <- ifelse(imbalance_data$days_to_death == "low risk", 0, 1)
imbalance_data$days_to_death <- as.factor(imbalance_data$days_to_death)

train_data_imbalance  <- imbalance_data[training_samples_imbalance, ]
test_data_imbalance <- imbalance_data[-training_samples_imbalance, ]

model_imbalance <- glm(days_to_death ~.,family=binomial,data=train_data_imbalance)

fitted_results <- predict(model_imbalance,subset(test_data_imbalance,select=-c(days_to_death)),type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)
misClasificError <- mean(fitted.results != test_data_imbalance$days_to_death)

print(paste('Test Accuracy:',1-misClasificError))

```





























































































