---
title: "LUAD"
author:
  - Gaye Çolakoğlu^[180709059]
  - Mehmet Cihan Sakman^[170709001]
date: "27 12 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

#LIBRARIES
We first load necessary libraries
```{r}
#install.packages("naniar")
#install.packages("cowplot") 
#install.packages("superml")
#install.packages("VIM")
#install.packages("smotefamily")
#install.packages("AICcmodavg")
library(factoextra)
library(pROC)
library(AICcmodavg)
library(smotefamily)
library(cluster)
library(VIM)
library(naniar)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(corrplot)
library(superml) #Label Encoding
library(caret) #dummyVars for One Hot Encoding
```
# DATA

### Load the Data
```{r}
missing.values <- c("N/A","na","?","", "NaN", "not reported", "NA")

data <- read.csv("C:\\Users\\sakma\\Desktop\\Ders Kayıtları\\Data Mining\\Final Project\\TCGA-LUAD_clinical.csv", na.strings=missing.values)

#data <- read.csv("C:\\Users\\Gaye\\OneDrive\\Masaüstü\\dersKayıtü\\DataScience\\DataScienceFinal\\TCGA-LUAD_clinical.csv", na.strings=missing.values)

```

## **1.2.3)** Data Analysis and Visualization
```{r }
summary(data)
```

As we can see from the summary() result our variables have different types of classes. Also, most of them include NA values.

Let's run str() to get information about types of features.
```{r cars}
str(data) #Types of features
```
From the str() result, we understand that our features includes different types of classes also we can see lots of NA values.


Let's take a final look at the structure and contents of our data in a visual and more entertaining way.

```{r}
library(visdat)
vis_dat(data)
```

As we can see from the table, we have many NA values, but some variables look full NA. We cannot say that these will be of use to us in the later stages.

# PREPROCESSING

At this step firstly we're going to drop the fully NaN columns. Because these columns are not useful for us. Maybe we can show these full nan columns in a fashionable way.

**we are going to get a rough glimpse on the missingness situation with the pretty neat naniar package**

Are there missing values in the dataset?
```{r}
any_na(data)
```
How many?
```{r}

n_miss(data)

```

What is the proportion of missing data?
```{r}
prop_miss(data)
```

# Which variables are affected?
```{r}
data %>% is.na() %>% colSums()

```

Drop full NA variables and append the new data frame to the data_new and take a look our new dataframe
```{r}

data_new <- data[ , colSums(is.na(data)) < nrow(data)]

summary(data_new)

```
Now we got rid of the full NA variables but we still have some variables that contain NA values.

Let's take a closer look at to the missing values with tables.
```{r}
# Get number of missings per variable (n and %)
miss_var_summary(data_new)

```

By looking at missing summary per variable, we notice that especially the “year_of_death” variable has the highest amount of missings among all variables in the dataset. The variable represents the “year of death of individuals.” 

We can transfer this summary to a visual representation like this:
```{r}
# Which variables contain the most missing variables?
gg_miss_var(data_new)
```

To get a better understanding we are going to visualize the locations of missing values across all variables.
```{r}
# Where are missings located?
vis_miss(data_new) + theme(axis.text.x = element_text(angle=80))
```

We can see where the missing values are “clustered” and it seems to match our findings from our previous overview on the presence of missing values per variable.

We can see that the presence of some values in some variables coincides with the absence of others. We can see the *days_to_last_follow_up* and *days_to_death* columns that are an example of missing with the existence of others.

Now is the presence of some values are related with missingness in other variables? Let’s find out.

```{r}
gg_miss_upset(data_new)
```

The results are compatible with the observation that there is a substantial number of cases in which some missingness happen to occur across certain variables.

From above result we can say that **There is a direct realtionship between *days_to_last_follow_up* and *days_to_death* columns**. Of course, there are other variables that meet this condition. But the binary that stood out compared to the numeric values was *days_to_last_follow_up* and *days_to_death*.

Before doing anything to missing values, we need to know the pattern of occurrence of missing values. The most effective way to do this is to draw a plot.

In order to examine the pattern between the variables containing the missing value by drawing a plot, let's take only the variables with at least one missing value from all the variables
```{r}
columns_with_na<-names(which(colSums(is.na(data_new)) > 0))
#columns_with_na

data_w_na <- subset(data_new, select = c(columns_with_na))
head(data_w_na)
```

Here first we change the variable name of *days_to_last_follow_up* to *days_to_lfu* in order to fit the name in the plot. This variable gives us information about how long alive people have their check-in and there is no information about the dead patients.

```{r}
names(data_w_na)[names(data_w_na) == 'days_to_last_follow_up'] <- 'days_to_lfu'
```

The missing patterns can be visualized using VIM package's **aggr** function.

```{r}
aggr(data_w_na, col=c('lightblue','red'),  sortVars=TRUE, labels=names(data_w_na), cex.axis=.5, gap=3, ylab=c("Histogram of missing data","Pattern"))

```
The histogram shows the proportion of missingness and the plot on the right gives the frequencies for different combinations of variables missing where. Blue refers to observed data and red to missing data.

From the above plot, again we can see presence of some values that related with missingness in other variables.

***
## Y.a) Missing Value Imputations

In the code below we'll drop the row, if there is a case like that if patient vital status is dead but days_to_death is 0.
As we can see we only have one patient with that situation in LUAD and we dropped that patient.
```{r}
paste("Before Missing value imputation we're going to keep the data with missing values as 'data_new_w_na'")
data_w_na <- data_new


filter(data_new, days_to_death == 0 & vital_status == "dead")
nrow(data_new)

drop_rows <-subset(data_new, (vital_status=="dead" & days_to_death==0))
c <-nrow(drop_rows)
paste("Count of patient with vital status is dead but days_to_death is 0 is:", c)

data_new <- data_new[!data_new$submitter_id %in% drop_rows$submitter_id,]
filter(data_new, days_to_death == 0 & vital_status == "dead")
nrow(data_new)


```

As we mentioned above **There is a direct realtionship between *days_to_last_follow_up* and *days_to_death* columns** but let's take a closer look to their relationship.

```{r}
to.death.vs.last.follow.up <- subset(data_new, select = c(days_to_last_follow_up,days_to_death))
aggr(to.death.vs.last.follow.up, col=c('lightblue','red'),  sortVars=TRUE, numbers=TRUE, cex.numbers=0.9,labels=names(to.death.vs.last.follow.up), cex.axis=.5, gap=3, ylab=c("Histogram of missing data","Pattern"))
```
From above plot we can say that the 63.1% of the missing values in **days_to_death** is present in **days_to_last_follow_up** and 22.5% of the missing values in **days_to_last_follow_up** is present in **days_to_death**. 
- Now, Now we'll fill the NaN values in days_to_death column with the days_to_last_follow_up. Subsequently, we're going to drop the days_to_last_follow_up.
```{r}
data_new$days_to_death[is.na(data_new$days_to_death)] <- data_new$days_to_last_follow_up[is.na(data_new$days_to_death)]  # Replace NA values
data$days_to_death[is.na(data$days_to_death)] <- data$days_to_last_follow_up[is.na(data$days_to_death)]  # Replace NA values 


#Drop the days_to_last_follow_up.
data_new <- subset(data_new, select = -c(days_to_last_follow_up))
data <- subset(data, select = -c(days_to_last_follow_up))
gg_miss_var(data_new)
```

As we can see we successfully merge these two features.

***

- We can drop year_of_birth column because we already have days_to_birth which is more suitable for our algorithm

- In 'state' column all the variables are same for all rows as 'released'. We'll drop it
```{r}
#unique_data <- sapply(data_new, function(x) length(unique(x)))
unique_data <- data_new %>% summarise_each(funs(n_distinct))
unique_data
```

- Drop updated_datetime because there is no special information about patient status

- Also we should remove the ids. Because there is nothing special for these ids. These are just random variables.
  - id_columns = {"diagnosis_id", "exposure_id", "demographic_id", "treatment_id", "bcr_patient_barcode"}
  
- 76% of the year_of_death column is missing. We're going to drop that column because we already have the information of days_to_death which refers to information of lifetime of the patient before dying. 
```{r}
data_new <- subset(data_new, select = -c(year_of_birth, state, updated_datetime, diagnosis_id, exposure_id, demographic_id, treatment_id, bcr_patient_barcode, year_of_death))
data <- subset(data, select = -c(year_of_birth, state, updated_datetime, diagnosis_id, exposure_id, demographic_id, treatment_id, bcr_patient_barcode, year_of_death,submitter_id))
```

After dropping unnecessary variables which is fully NA and variables that contain only the same values, this is our last version of data.

```{r}
library(visdat)
vis_dat(data_new)
```
***


#### Now we will fill the numeric NaN variables with the mean of columns.

- numeric_columns: *age_at_diagnosis*, *days_to_birth*, *years_smoked*, *cigarettes_per_day*
```{r}
## Try to do the same operation with less code. Try sth fancy.
data_new$age_at_diagnosis[is.na(data_new$age_at_diagnosis)] <- mean(data_new$age_at_diagnosis, na.rm = TRUE)
data_new$days_to_birth[is.na(data_new$days_to_birth)] <- mean(data_new$days_to_birth, na.rm = TRUE)
data_new$years_smoked[is.na(data_new$years_smoked)] <- mean(data_new$years_smoked, na.rm = TRUE)
data_new$cigarettes_per_day[is.na(data_new$cigarettes_per_day)] <- mean(data_new$cigarettes_per_day, na.rm = TRUE)

# Which variables contain the most missing variables?
gg_miss_var(data_new)
```

We can see that we successfully imputed these features with mean of the features.

- Now we have null values in **ethnicity** and **race** columns. These columns are highly related with cancer genetic we will keep them as unknown
```{r}
 
head(data_new)

data_new <- data_new %>%
    mutate(ethnicity = if_else(is.na(ethnicity), "unknown", ethnicity))
data_new <- data_new %>%
    mutate(race = if_else(is.na(race), "unknown", race))

head(data_new)
```

  - We can show the new distribution of the **ethnicity** and **race** columns in a plot.
```{r}
p1 <- ggplot(data_new, aes(x=reorder(ethnicity, ethnicity, function(x)-length(x)))) +
geom_bar(fill='orange') +  labs(x='Ethnicity', y="")
p2 <- ggplot(data_new, aes(x=reorder(race, race, function(x)-length(x)))) +
geom_bar(fill='purple') +  labs(x='Race', y="")

require(gridExtra)
grid.arrange(p1, p2, nrow=2)


```



(There are still some NaN values in 'tumor_stage'(8) and 'days_to_death'(9). We'll drop them)
```{r}
sapply(data_new,function(x) sum(is.na(x)))
vars <- c("tumor_stage", "days_to_death")
data_new <- data_new %>% drop_na(vars, any_of(vars))
sapply(data_new,function(x) sum(is.na(x)))
```

As we can see, we don't have any null values in the dataset anymore.
```{r}
gg_miss_var(data_new)
```

## 4) Multicollinearity

- Let's plot the Correlation Coefficient Matrix for multicollinearity check. But we can apply the Correlation Coefficient Matrix only to numeric columns. Also, we need to check for non-numeric features for multicollinearity. We're going to use a function to check if there are identical columns in the data frame with non-numeric columns. To be able to check for identical features, we're going to print a list that consists of two columns. If a feature has an identical feature second column will show the feature which is identical to the first one.
```{r}
matrix <- data_new %>%
    select_if(is.numeric) %>%
    cor(.)
corrplot(matrix, method = "color", addCoef.col="black", order = "alphabet", number.cex=0.6, title = "Correlation Coefficient Matrix",
         tl.cex=0.6, tl.col = "black")

tt <- lapply(unique(as.list(data_new)), function(x) {colnames(data_new)[as.list(data_new) %in% list(x)]})
t(sapply(tt, "length<-", max(lengths(tt))))
```
**Note**: The above correlation matrix shows that there are no highly correlated features and due to there are no multicollinearity for numeric features. On the other hand, for the non-numeric features it seems that tissue_or_organ_of_origin and site_of_resection_or_biopsy are identical features. Therefore, we're going to drop *tissue_or_organ_of_origin*.

```{r}
data_new <- subset(data_new, select = -c(tissue_or_organ_of_origin))
```


## ENCODING

- We're going to convert our String Binary columns into Numeric Binary columns with LabelEncdoing. But first we're going to convert these three Character features into Factor.
  - Our Binary String Columns are: vital_status, gender, disease
```{r}

data_new$vital_status <- as.factor(data_new$vital_status) 
data_new$gender <- as.factor(data_new$gender) 
data_new$disease <- as.factor(data_new$disease) 

head(data_new)

label <- LabelEncoder$new()
data_new$vital_status <- label$fit_transform(data_new$vital_status) #0:Alive, 1:Dead
data_new$gender <- label$fit_transform(data_new$gender) #1:Male, 0:Female
data_new$disease <- label$fit_transform(data_new$disease) #We're going to drop it.

head(data_new)
```
- **tumor_stage** is an ordinal column. We'll encode it manuelly. We can get the unique values for the feature and show them.
  - The order is going to be as follows:
    - tumor_stage_map = {'stage i': 1, 'stage ia': 2, 'stage ib': 3, 'stage ii': 4, 'stage iia': 5, 'stage iib': 6, 'stage iiia':7, 'stage iiib':8, 'stage iv':9 }

```{r}
head(data_new)
p1 <- ggplot(data_new, aes(x=reorder(tumor_stage, tumor_stage, function(x)-length(x)))) +
geom_bar(fill='orange') +  labs(x='Tumor Stages', y="")
p1


data_new$tumor_stage <- as.numeric(as.factor(data_new$tumor_stage))


str(data_new$tumor_stage)
typeof(data_new$tumor_stage)
head(data_new)
```



- We'll apply OneHotEncoding for Nominal Columns with **dummyVars** function in the *caret* package.
  - Our categorical columns are: **primary_diagnosis, morphology, site_of_resection_or_biopsy, race, ethnicity**
```{r}
categorical.cols <- subset(data_new, select=c(primary_diagnosis, morphology, site_of_resection_or_biopsy, race, ethnicity))
head(categorical.cols)

dmy <- dummyVars(" ~ .", data = categorical.cols)
categorical.cols <- data.frame(predict(dmy, newdata = categorical.cols))
head(categorical.cols)
```

  - We'll drop categorical columns and then we'll merge the new One Hot Encoded version of categorical.cols to the dataset.
```{r}
categorical.cols$submitter_id <- data_new$submitter_id #We need that submitter_id column to merge two dataframes.
data_new <- subset(data_new, select = -c(primary_diagnosis, morphology, site_of_resection_or_biopsy, race, ethnicity))
data_new <- merge(data_new, categorical.cols, by="submitter_id")
```
  
  
- Drop vital_status(alive or dead) and disease. In real life we don't need vital_status feature. On the other hand **disease** stands for type of Lung Cancer(LUAD or LUSC) in our case all the patients are LUAD cancer, therefore we don't need to keep that column. Also we do not need **submitter_id** feature any more.
```{r}
data_new <- subset(data_new, select = -c(vital_status, disease, submitter_id))
```

## Classifiyng Algorithm

We would like to classify the patients such as they will live more than five years or not according to their *days_to_death* feature. Therefore, if the patient's remaining life more than 5 years we assumed the patient will live long(assign 1), otherwise short(assign 0). Then convert it to the factor.
```{r}
head(data_new)
data_new$days_to_death <- ifelse(data_new$days_to_death>1825, 'low risk', 'high risk')
data_new$days_to_death <- as.factor(data_new$days_to_death)
head(data_new)
str(data_new)

data$days_to_death <- ifelse(data$days_to_death>1825, 0, 1)
data$days_to_death <- as.factor(data$days_to_death)
```




## Z.a) Imbalance Check

## Let's see if we have a balanced classes or not for the target data(*days_to_death*).
```{r}
p1 <- ggplot(data_new, aes(x=reorder(days_to_death, days_to_death, function(x)-length(x)))) +
geom_bar(fill='orange') +  labs(x='Classes', y="")
p1
```

It seems that we have a serious imbalance class classification problem. We're going to use the SMOTE algorithm, the SMOTE is an oversampling technique which synthesizes a new minority instance between a pair of one minority instance and one of its K nearest neighbor.
```{r}
paste("We're going to keep the imbalance data as 'imbalance_data' and 'data_new' is going to be our balanced data.")

imbalance_data <- data_new


data_new <- SMOTE(subset(data_new, select = -c(days_to_death)),subset(data_new, select = c(days_to_death)),K=5)$data



names(data_new)[names(data_new) == 'class'] <- 'days_to_death'

set.seed(12345)
p1 <- ggplot(data_new, aes(x=reorder(days_to_death, days_to_death, function(x)-length(x)))) +
geom_bar(fill='orange') +  labs(x='Classes', y="Count") + ggtitle("Balanced Data")
p1
```

After applying SMOTE now we have a balanced data. 

## 5) PCA

First, we apply PCA keeping all components equal to the original number of dimensions and see how well PCA captures the variance of our data.
```{r}
X_data <- subset(data_new, select = -c(days_to_death))
y_data <- subset(data_new, select = c(days_to_death)) # our target

#Now we can apply PCA:
X_data_pca <- prcomp(X_data,center = TRUE, scale. = TRUE)
summary(X_data_pca)

```

Because the point of PCA is to significantly reduce the number of variables, we want to use the smallest number of principal components possible to explain most of the variability.

The most common technique for determining how many principal components to keep is eyeballing the scree plot. In a scree plot the ‘arm-bend’ represents a decrease in cumulative contribution also it show the percentage of variances explained by each principal component.
```{r}
screeplot(X_data_pca, type = "l", npcs = 25)
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Eigenvalue = 1"),
       col=c("red"), lty=5, cex=0.6)

```

As you see above we have a line which name is Eigenvalue we can say eigenvalue is the standard deviation of our PC’s and we just take the first 20 PC’s which their eigenvalues greater than 1.

The above plot shows the bend at the eighteenth principal component, it is useful to decide how many PCs to retain for further analysis. In this case we have 18 PCs and we can see that the first 18 PCs explain most of the variability in the data.

```{r}
cumpro <- cumsum(X_data_pca$sdev^2 / sum(X_data_pca$sdev^2))
plot(cumpro[0:30], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative Variance")
abline(v = 20, col="blue", lty=5)
abline(h = 0.89271, col="blue", lty=5)
legend("topleft", legend=c("Cut-off @ PC6"),
       col=c("blue"), lty=5, cex=0.6)
```

From above plot we can see that nearly with 20 PCs we can explain %89.271 of total variance.

Let's see the importance of first 20 PCs out of 42:
```{r}
X_data_pca_20 <- prcomp(subset(X_data), center = TRUE, scale = TRUE, rank.= 20)
summary(X_data_pca_20)
```

From above summary, let's look at the cumulative proportion of PC20. It's value is 0.89 which means that we can explain %0.89 of total variance with 20 PCs out of 42.

Now lets look at correlation again.
```{r}
M <-cor(X_data_pca_20$x)
corrplot(M, method = "color", addCoef.col="brown", order = "FPC", number.cex=0.9, 
         tl.cex=0.7, tl.col = "brown", tl.srt = 90)

```

As you see above there is no correlation between our PC’s.That’s what we have wanted.

Now we'll keep new data with name 'transformed_data' which have fewer columns to create a more powerful model
```{r}
transformed_data<- cbind(X_data_pca_20$x,y_data)
head(transformed_data)
```

## 6) LOGISTIC REGRESSION
```{r}
training_samples <- data_new$days_to_death %>% createDataPartition(p = 0.8, list = FALSE)

#First we convert our days_to_death column to numeric
data_new$days_to_death <- ifelse(data_new$days_to_death == "low risk", 0, 1)
data_new$days_to_death <- as.factor(data_new$days_to_death)

#Then we split our data into train and test data.
train_data  <- data_new[training_samples, ]
test_data <- data_new[-training_samples, ]

#Now we are creating our model.
model <- glm(days_to_death ~.,family=binomial,data=train_data)

#Let’s see what it can be able to our model?
fitted_results <- predict(model,subset(test_data,select=-c(days_to_death)),type='response')
fitted_results <- ifelse(fitted_results > 0.5,1,0)

confusionMatrix(test_data$days_to_death,as.factor(fitted_results))

misClasificError <- mean(fitted_results != test_data$days_to_death)

print(paste('Test Accuracy:',1-misClasificError))


```

As you see above model accuracy is 67%. It is good but let's see if we can increase this ratio with PCA dataset(transformed data) which we created above.

## 6.X) LOGISTIC REGRESSION WTIH PCA

Let's create new model but this time with PCA then we are going to compare the result with model that that is created at step 6.
```{r}
training_samples_pca <- transformed_data$days_to_death %>% createDataPartition(p = 0.8, list = FALSE)

#First we convert our days_to_death column to numeric
transformed_data$days_to_death <- ifelse(transformed_data$days_to_death == "low risk", 0, 1)
transformed_data$days_to_death <- as.factor(transformed_data$days_to_death)

#Then we split our data into train and test data.
train_data_pca  <- transformed_data[training_samples_pca, ]
test_data_pca <- transformed_data[-training_samples_pca, ]

#Now we can build our linear regression model with our new data
model_pca <- glm(days_to_death ~.,family=binomial,data=train_data_pca)

#Let’s see what it can be able to our model?
fitted_results_pca <- predict(model_pca,subset(test_data_pca,select=-c(days_to_death)),type='response')
fitted_results_pca <- ifelse(fitted_results_pca > 0.5,1,0)

confusionMatrix(test_data_pca$days_to_death,as.factor(fitted_results_pca))

misClasificError_pca <- mean(fitted_results_pca != test_data_pca$days_to_death)

print(paste('Test Accuracy:',1-misClasificError_pca))
```

# Compare result of 6 and 6.X:

**Result** From above result we can see that the model_pca accuracy is 74% while model accuracy is 68%. Also we can say that the model with applying PCA is more useful than the model without PCA in our data analysis.

## 6.Y.b) LOGISTIC REGRESSION WTIH MISSING VALUES
This time we want to create our model with missing values then we are going to compare the result with model that do not contain missing values which is created at step 6.
```{r}
#library(rpart)
#install.packages("rpart.plot")
#library(rpart.plot)

#training_samples_missing <- data$days_to_death %>% createDataPartition(p = 0.8, list = FALSE)

#Then we split our data into train and test data.
#train_data_missing  <- data[training_samples_missing, ]
#test_data_missing <- data[-training_samples_missing, ]

#model_missing <- rpart(days_to_death ~., data = train_data_missing, cp=0.07444)

#Let’s see what it can be able to our model?
#fitted_results_missing <- predict(model_missing,subset(test_data_missing,select=-c(days_to_death)),type='response') 
```
**Note:** It doesn't work with data because we didn't compute the Encoding steps for the data with NA values.....

# Compare result of 6 and 6.Y.b:
```{r}
#models <- list(model, model_missing)

#model.names <- c('model', 'model_missing')
#aictab(cand.set = models, modnames = model.names)
```

## 6.Z.b) LOGISTIC REGRESSION WTIH IMBALANCED DATA
Let's also create our model with imbalanced data then we are going to compare the result with model that contain balanced data which is created at step 6.
```{r}
training_samples_imbalance <- imbalance_data$days_to_death %>% createDataPartition(p = 0.8, list = FALSE)

#First we convert our days_to_death column to numeric
imbalance_data$days_to_death <- ifelse(imbalance_data$days_to_death == "low risk", 0, 1)
imbalance_data$days_to_death <- as.factor(imbalance_data$days_to_death)

#Then we split our data into train and test data.
train_data_imbalance  <- imbalance_data[training_samples_imbalance, ]
test_data_imbalance <- imbalance_data[-training_samples_imbalance, ]

#Now we can build our linear regression model with our imbalanced data
model_imbalance <- glm(days_to_death ~.,family=binomial,data=train_data_imbalance)

#Let’s see what it can be able to our model?
fitted_results_imbalance <- predict(model_imbalance,subset(test_data_imbalance,select=-c(days_to_death)),type='response')
fitted_results_imbalance <- ifelse(fitted_results_imbalance > 0.5,1,0)

confusionMatrix(test_data_imbalance$days_to_death,as.factor(fitted_results_imbalance))

misClasificError_imbalance <- mean(fitted_results_imbalance != test_data_imbalance$days_to_death)

print(paste('Test Accuracy:',1-misClasificError_imbalance))

```

Despite the accuracy being 90%, the Sensitivity is 50%. That means our algorithm only predicts the patients with High Risk. It can not predict the Low-Risk patient well. As you can see from the confusion matrix it correctly classified only one patient out of ten which is a Low-Risk patient.

# Compare result of 6 and 6.Z.b:

We fitted the imbalanced data with Logistic Regression and using the Accuracy is not a good metrics for measuring the algorithm's success on the imbalance data. Therefore we're going to plot the ROC curve(AUC) to show the success of the algorithm.
```{r}
fitted_results_imbalance <- predict(model_imbalance, subset(test_data_imbalance,select=-c(days_to_death)),type='response')
auc <- roc(test_data_imbalance$days_to_death, fitted_results_imbalance)
print(auc)
p1 <- plot(auc, ylim=c(0,1), print.thres=TRUE, main=paste('AUC:',round(auc$auc[[1]],2)))
abline(h=1,col='blue',lwd=2)
abline(h=0,col='red',lwd=2)
#models <- list(model, model_imbalance)

#model.names <- c('model', 'model_imbalance')
#aictab(cand.set = models, modnames = model.names)
```

As we can see from above the AUC is 0.66 which is a really bad result. Now let's plot the AUC for the balanced data and see if the SMOTE improved the performance of the algorithm or not.
```{r}
fitted_results <- predict(model,subset(test_data,select=-c(days_to_death)),type='response')
auc <- roc(test_data$days_to_death, fitted_results)
print(auc)
p1 <- plot(auc, ylim=c(0,1), print.thres=TRUE, main=paste('AUC:',round(auc$auc[[1]],2)))
abline(h=1,col='blue',lwd=2)
abline(h=0,col='red',lwd=2)
```

Applying the SMOTE algorithm to our data improved the Logistic Regression's performance from 0.66 to 0.77. It proves the power of the OverSampling with SMOTE.

Based on this comparison, we would choose the balanced data to use in our data analysis.

***

## **7)** Clustering Algorithms

- **a)** Clustering is powerful technique when it's about unsupervised learning. Some of the clustering algorithms directly clusters the data points without any given information but some of them need to have the information of number of clusters. In order to show the difference between these two type clustering we're going to apply K-means and Hierarchical Clustering Algorithms. 

- **7.1)** KNN Clustering

Let's first try to show the distribution of the days_to_death using **cigarettes_per_day** and **years_smoked**
```{r}
cigarettes.per.day <- data_new$cigarettes_per_day
years.smoked <- data_new$years_smoked
ggplot(data_new, aes(cigarettes.per.day, years.smoked, color = days_to_death)) + geom_point()
```
Due to we imputed to null values with mean values of the feature in **years_smoked** there is a line on the middle of the y-axis. When we look at the above plot and try to find a pattern, it could be a little bit hard but we can say that high-risk patients spread wider than low-risk patients. But clustering algorithms will suffer to identify the clusters. Let's try it.

We are going to assign the # of clusters to 2 to identify the **high risk** and **low risk** patients.

- We’re going to separate the target(days_to_death) before applying k-means.
```{r}
x_data <- subset(data_new, select = -c(days_to_death))
y_data <- subset(data_new, select = c(days_to_death))

kmeans_data <- kmeans(x_data, center=2, nstart=30)
#kmeans_data$centers
```

Let's try to visualize the clusters on the CigarettesPerDay vs YearsSmoked plot.
```{r}
## Visualizing clusters
y_kmeans <- kmeans_data$cluster
clusplot(x_data[, c("cigarettes_per_day", "years_smoked")],
         y_kmeans,
         lines = 0,
         shade = TRUE,
         color = TRUE,
         labels = 2,
         plotchar = FALSE,
         span = TRUE,
         main = paste("Cluster Risk Levels"),
         xlab = 'Years Smoked',
         ylab = 'Cigarettes Per Day')
```
It could not identify the clusters well according to CigarettesPerDay and YearsSmoked. Let's try to plot clusters and see if we can find out anything.
```{r}
fviz_cluster(kmeans_data, data = x_data)
```
As we can see again due to our classes generated synthetically with SMOTE algorithm the classes are so close to each others and clustering algorithm suffers to identify bounds of the classes.

- Now let's compare the predicted values and actual values
```{r}
#install.packages("cvms")
library(cvms)
library(factoextra)

table(kmeans_data$cluster, y_data$days_to_death)
```
**Note**: As we can see from the above clustering algorithms didn't predict well. For the low risk, the algorithm almost has 50% accuracy and it means nothing for the Machine Learning algorithm. Rather than using that algorithm we can just flip a coin and decide whether it's low or high risk.

- **7.2)** HIERARCHICAL CLUSTERING(HC)

Let's plot the elbow method and see what is suggested number of clusters. 
```{r}
#Agglomerative hierarchical clustering (HC)---hclust function
distance <- dist(subset(data_new, select = -c(days_to_death)), method="euclidean") 
#Elbow method can also be used here

fviz_nbclust(subset(data_new, select = -c(days_to_death)), FUN = hcut, method = "wss")

```

It seems that we should try to have three clusters. Let's plot the dendogram tree. The height of the cut to the dendrogram controls the number of clusters obtained and we're going to try to cut the tree with different heights. Let's first cut it into 2 clusters.
```{r}
hier <- hclust(distance, method="average")
plot(hier,  xlab="", sub=" ") 
rect.hclust(hier, k=2, border="red")
```

```{r}
hier_cut <- cutree(hier, 2)
table(predicted=hier_cut, true=data_new$days_to_death)
```
With the number of two clusters it can not identify the clusters well as we can see it also from the dendogram and we could have more clusters.

Let's try it for three clusters.
```{r}

plot(hier,  xlab="", sub=" ") 
rect.hclust(hier, k=3, border="red")

hier_cut <- cutree(hier, 3)
table(predicted=hier_cut, true=data_new$days_to_death)
```

Now from one of the clusters identfy the class 1 much better than before. Let's try it with 4 cluster.
```{r}
plot(hier,  xlab="", sub=" ") 
rect.hclust(hier, k=4, border="red")

hier_cut <- cutree(hier, 4)
table(predicted=hier_cut, true=data_new$days_to_death)
```

It seems that increasing the number of clusters doesn't work anymore. Because the new cluster only consists of one individual and it doesn't mean anything to us.

**Result:** We can not identify the target classes well with Hierarchical Clustering algorithm. It can correctly classify almost *66%* of the class 1 but it didn't work well for the class 0. 

- **b)** In this situation comparing these two Clustering algorithms doesn't make sense because the selected two algorithms couldn't come up with good classification accuracies too. But we can say that Hierarchical Clustering did a better job of predicting Class 1 than K-means. But both of them could not identify Class 0 well.

***

## **8**) Apply at least 2 Classification Techniques

- **a)** We're going to apply the two popular classifying algorithms, Random Forest and Logistic Regression. We choose the Random Forest because it handles the overfitting if it exists very well and also deals with a large number of features. Due to we have 45 features we'll use the Random Forest's power for our benefit. Logistic Regression is very easy to use and it suits the binary class classification problems which we are dealing with. Logistic Regression works a bit faster than other classification algorithms. Due to our having balanced data, we're going to check the Accuracy which is basically the ratio of correct classified classes to total classes.

- **8.1)** Logistic Regression: We have already applied the Logistic Regression in **Step 6** then we can move forward with step 8.2.

- **8.2)** Random Forest: We're going to apply the powerful classifying algorithm Random Forest to try to predict whether the patient is a high risk or low risk.

- We'll first fit and train our data with 500 trees which is default for Random Forest Algorithm and then we'll increase the number of trees and try to observe the change on the accuracy. 
```{r}
library(randomForest)
rf <- randomForest(days_to_death~., data=train_data, proximity=TRUE, ntree = 500)
print(rf)
```

Our out of bag(OOB) error is 5%, so the train data set model accuracy is around 95%.

Now let's make a prediction with the train_data and see the performance.
```{r}
p1 <- predict(rf, train_data)
confusionMatrix(p1, train_data$days_to_death)
```

The performance of training data is 98% almost all patient's classified correctly. Now let's test it with unseen test_data.
```{r}
p1 <- predict(rf, test_data)
confusionMatrix(p1, test_data$days_to_death)
```

We have a very high accuracy with test data with 94%. 

- Now let's plot the top 10 most important features and see the effects of them.
```{r}
varImpPlot(rf,
           sort = T,
           n.var = 10,
           main = "Top 10 - Variable Importance")
```

It seems that the *ethnicity* column is the most important feature for the Random Forest algorithm.

- Now try to increase the number of trees from 500 to 1000 in the Random Forest and see the changes in the accuracy.
```{r}
rf <- randomForest(days_to_death~., data=train_data, proximity=TRUE, ntree = 1000)
p1 <- predict(rf, test_data)
confusionMatrix(p1, test_data$days_to_death)
```

After increasing the number of trees from 500 to 1000 we couldn't improve the accuracy.

- **b)** We have *74%* Accuracy in Logistic Regression and *94%* with Random Forest. There is a huge difference between these two algorithms. Due to we have a large number of features and Random Forest is a little bit more sophisticated algorithm that could be why we have much better accuracy with Random Forest compared to Logistic Regression.
























































































